{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rXusCeRVoiV_"
      },
      "outputs": [],
      "source": [
        "# dependencies\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJawvOFwv7Zd",
        "outputId": "fb2d5c20-4ae2-49a2-a9e5-bc3974331b12"
      },
      "outputs": [],
      "source": [
        "# Set the path to the file you'd like to load\n",
        "file_path = \"products.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.dataset_load(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"poorveshchaudhari/amazon-fashion-products\",\n",
        "    file_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nks2GtxkofQh"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=[\"image_url\", \"rating\"])\n",
        "df = df[[\"image_url\", \"rating\"]]\n",
        "train_df, test_df = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "train_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_DIR = \"images\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "def download_images(df):\n",
        "    for i, url in tqdm(enumerate(df[\"image_url\"]), total=len(df)):\n",
        "        filename = os.path.join(IMG_DIR, f\"{i}.jpg\")\n",
        "        if not os.path.exists(filename):  # skip if already downloaded\n",
        "            try:\n",
        "                r = requests.get(url, timeout=10)\n",
        "                if r.status_code == 200:\n",
        "                    with open(filename, \"wb\") as f:\n",
        "                        f.write(r.content)\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {url}: {e}\")\n",
        "\n",
        "download_images(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3AUAutYVpsW-"
      },
      "outputs": [],
      "source": [
        "# prepare the data\n",
        "\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def fetch_images(path, label):\n",
        "    path = path.numpy().decode('utf-8')\n",
        "\n",
        "    try:\n",
        "        img = tf.keras.utils.load_img(path, target_size = IMG_SIZE)\n",
        "        img = tf.keras.utils.img_to_array(img) / 255.0 # since every pixel has values [0, 255], dividing by 255 normalizes them to [0, 1]\n",
        "    except Exception:\n",
        "        img = np.zeros((*IMG_SIZE, 3), dtype = np.float32)\n",
        "\n",
        "    return img.astype(np.float32), np.float32(label)\n",
        "\n",
        "def tf_fetch_image(url, label):\n",
        "    img, lbl = tf.py_function(fetch_images, [url, label], [tf.float32, tf.float32])\n",
        "    img.set_shape((*IMG_SIZE, 3))\n",
        "    lbl.set_shape(())\n",
        "\n",
        "    return img, lbl\n",
        "\n",
        "def make_datasets(sub_dataframe):\n",
        "    paths = [os.path.join(IMG_DIR, f\"{i}.jpg\") for i in sub_dataframe.index]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, sub_dataframe[\"rating\"].values))\n",
        "    ds = ds.map(tf_fetch_image, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    ds = ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds, val_ds, test_ds = make_datasets(train_df), make_datasets(val_df), make_datasets(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UXF2YaTOrMSX"
      },
      "outputs": [],
      "source": [
        "# here we can visualize the data (optional)\n",
        "\n",
        "\"\"\"\n",
        "the first for loop displays:\n",
        "\n",
        "(32, 224, 224, 3) [3.3, ...., 1.5]\n",
        "\n",
        "Here we have:\n",
        "- a batch of 32 images\n",
        "- 224x224 each (and their 3 respetive channels)\n",
        "- [3.3, ..., 1.5] the array of ratings for each image\n",
        "\"\"\"\n",
        "\n",
        "for img, lbl in train_ds.take(1):\n",
        "    print(img.shape, lbl.numpy())\n",
        "\n",
        "\"\"\"\n",
        "second for loop shows the values for each pixel\n",
        "\n",
        "For each image we have:\n",
        "- first axis: rows (height = 224)\n",
        "- second axis: columns (width = 224)\n",
        "- third axis: channels (RGB = 3)\n",
        "\"\"\"\n",
        "for imgs, labels in train_ds.take(1):\n",
        "    print(\"Image shape:\", imgs[1].shape)\n",
        "    print(\"Pixel values (first image):\")\n",
        "    print(imgs[1].numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkY0Tsibfe0u"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(hp):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # input layer for images\n",
        "    model.add(tf.keras.layers.Input(shape=(128, 128, 3)))\n",
        "\n",
        "    # hyperparameter: number of conv layers, 1–3\n",
        "    for i in range(hp.Int(\"num_conv_layers\", 1, 3)):\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2D(\n",
        "            filters = hp.Int(f\"filters_{i}\", min_value = 32, max_value = 128, step = 32),\n",
        "            kernel_size = hp.Choice(\"kernel_size\", values = [3, 5]),\n",
        "            activation = \"relu\",\n",
        "            padding = \"same\"\n",
        "        ))\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # flatten to connect with dense layers\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    # hyperparameter: number of dense layers, 0–2\n",
        "    for j in range(hp.Int(\"num_dense_layers\", 0, 2)):\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units = hp.Int(f\"units_dense_{j}\", min_value = 64, max_value = 256, step = 64),\n",
        "            activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        ))\n",
        "\n",
        "    # output layer (regression on rating)\n",
        "    model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "    # hyperparameter: learning rate\n",
        "    hp_lr = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
        "        loss = \"mse\",\n",
        "        metrics = [\"mae\", rmse, r2_score]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0eYOlD1f7gQ",
        "outputId": "7e66358f-9c7e-4b38-e6fc-69382de701dd"
      },
      "outputs": [],
      "source": [
        "build_cnn_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ7qld1Pg40t",
        "outputId": "b60a7644-afa8-4dd0-e7a4-9da242056daa"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel = build_cnn_model,\n",
        "    objective = \"val_mae\",\n",
        "    max_trials = 10,\n",
        "    directory = \"my_dir\",\n",
        "    project_name = \"cnn_tuning\",\n",
        "    overwrite = True\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ovifHMVXhIkl",
        "outputId": "d47000de-29a1-4888-fe6e-742f837f45b5"
      },
      "outputs": [],
      "source": [
        "tuner.search(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = 5,\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JejtU_mpiYOm"
      },
      "outputs": [],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHGagbXxifhN"
      },
      "outputs": [],
      "source": [
        "# extract the best model\n",
        "best_model = tuner.get_best_models(num_models = 1)[0]\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDe7HCK8iiEC"
      },
      "outputs": [],
      "source": [
        "loss, mae, rmse_val = best_model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test MAE: {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse_val:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
